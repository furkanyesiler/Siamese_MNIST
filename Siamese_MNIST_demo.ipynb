{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(nn.Module):\n",
    "    \"\"\"\n",
    "        The class for a siamese network. The network contains:\n",
    "            1 - 3 convolutional layers with ReLU non-linearization\n",
    "            2 - 2 max-pooling layers in between first and second, \n",
    "                and second and third convolutional layers\n",
    "            3 - 2 fully connected layers\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Initializing the network        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 2)\n",
    "        \n",
    "        self.lin1 = nn.Linear(1152, 512)\n",
    "        self.lin2 = nn.Linear(512, 64)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "            Defining a forward pass of the network\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            data : torch.Tensor\n",
    "                Input tensor for the network\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            x : torch.Tensor\n",
    "                Output tensor from the network\n",
    "                \n",
    "        \"\"\"\n",
    "        # passing the data through first convolutional layer \n",
    "        # and applying non-linearization with ReLU\n",
    "        x = self.conv1(data)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # max-pooling\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # passing the data through second convolutional layer \n",
    "        # and applying non-linearization with ReLU\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # max-pooling\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # passing the data through third convolutional layer \n",
    "        # and applying non-linearization with ReLU\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # flattening the data for fully connected layers\n",
    "        x = x.view(-1,1152)\n",
    "        \n",
    "        # passing the data through first and second fully connected layers\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_batch(dataset, idx_label_array, start_idx, end_idx, pair_set):\n",
    "    \"\"\"\n",
    "        Generating batches for training the siamese network. For each sample,\n",
    "        this method finds a genuine and an impostor sample to create pairs. The\n",
    "        genuine and impostor samples are selected randomly from the dataset.\n",
    "        pair_set is used to keeping track of the selected pairs so that in the \n",
    "        next batches, the same pairs are not used.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : torchvision.datasets.mnist.MNIST\n",
    "            Dataset used to creating batches\n",
    "        idx_label_array : numpy.ndarray\n",
    "            Labels of all the samples in the dataset\n",
    "        start_idx : int\n",
    "            Number of comma values to divide between 0 and 1200 cent\n",
    "        end_idx : int\n",
    "            Whether to include the first and the last sections\n",
    "        pair_set : set\n",
    "            Set of index pairs\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        batch_1_x : torch.Tensor\n",
    "            Values of the first batch\n",
    "        batch_2_x : torch.Tensor\n",
    "            Values of the second batch\n",
    "        batch_1_y : torch.Tensor\n",
    "            Labels of the first batch\n",
    "        batch_2_y : torch.Tensor\n",
    "            Labels of the second batch\n",
    "        batch_c : torch.Tensor\n",
    "            Contrastive labels for the pairs\n",
    "        pair_set : set\n",
    "            Updated set of index pairs\n",
    "        \n",
    "    \"\"\"\n",
    "    # size of the current batch\n",
    "    batch_size = end_idx - start_idx\n",
    "    \n",
    "    # initializing batches\n",
    "    batch_1_x = np.zeros((batch_size*2, 1, 28, 28))\n",
    "    batch_2_x = np.zeros((batch_size*2, 1, 28, 28))\n",
    "    batch_1_y = np.zeros((batch_size*2))\n",
    "    batch_2_y = np.zeros((batch_size*2))\n",
    "    \n",
    "    # array for containing contrastive labels (e.g. 0 if genuine pair, 1 if impostor)\n",
    "    batch_c = np.zeros((batch_size*2))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # index of the current sample\n",
    "        idx = i + start_idx\n",
    "        \n",
    "        # label of the current sample\n",
    "        label = dataset[idx][1].item()\n",
    "        \n",
    "        # index of a random selected sample with the same label (for genuine pair)\n",
    "        g_index = np.random.choice(np.where(idx_label_array==label)[0], size=1)[0]\n",
    "        \n",
    "        # index of a random selected sample with a different label (for impostor pair)\n",
    "        i_index = np.random.choice(np.where(idx_label_array!=label)[0], size=1)[0]\n",
    "        \n",
    "        # checking if the selected genuine pair is selected before\n",
    "        while set((idx, g_index)) in pair_set:\n",
    "            g_index = np.random.choice(np.where(idx_label_array==label)[0], size=1)[0]\n",
    "            \n",
    "        # checking if the selected impostor pair is selected before\n",
    "        while set((idx, i_index)) in pair_set:\n",
    "            i_index = np.random.choice(np.where(idx_label_array!=label)[0], size=1)[0]\n",
    "        \n",
    "        # adding the current pairs to pair_set\n",
    "        pair_set.add(frozenset((idx, g_index)))\n",
    "        pair_set.add(frozenset((idx, i_index)))\n",
    "        \n",
    "        # adding the values and labels of the genuine pair to batches\n",
    "        batch_1_x[i*2] = dataset[idx][0]\n",
    "        batch_2_x[i*2] = dataset[g_index][0]\n",
    "        batch_1_y[i*2] = label\n",
    "        batch_2_y[i*2] = label\n",
    "        \n",
    "        # setting the contrastive label for the genuine pair\n",
    "        batch_c[i*2] = 0\n",
    "        \n",
    "        # adding the values and labels of the impostor pair to batches\n",
    "        batch_1_x[i*2+1] = dataset[idx][0]\n",
    "        batch_2_x[i*2+1] = dataset[i_index][0]\n",
    "        batch_1_y[i*2+1] = label\n",
    "        batch_2_y[i*2+1] = dataset[i_index][1].item()\n",
    "        \n",
    "        # setting the contrastive label for the impostor pair\n",
    "        batch_c[i*2+1] = 1\n",
    "    \n",
    "    # casting the batches from numpy arrays to float tensors\n",
    "    batch_1_x = torch.from_numpy(batch_1_x).float()\n",
    "    batch_2_x = torch.from_numpy(batch_2_x).float()\n",
    "    batch_1_y = torch.from_numpy(batch_1_y).float()\n",
    "    batch_2_y = torch.from_numpy(batch_2_y).float()\n",
    "    batch_c = torch.from_numpy(batch_c).float()\n",
    "        \n",
    "    return batch_1_x, batch_2_x, batch_1_y, batch_2_y, batch_c, pair_set  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_batch(dataset, sample_idx):\n",
    "    \"\"\"\n",
    "        Generating batches for testing the siamese network. In order to do that, this function \n",
    "        creates pairs of samples to feed it in to the network. For ith element in the dataset \n",
    "        with the size N, this function creates pairs of ith element and (i+1)th element, ..., Nth \n",
    "        element of the dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : torchvision.datasets.mnist.MNIST\n",
    "            Dataset used to creating batches\n",
    "        sample_idx : int\n",
    "            Index of the current sample to create a batch\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        batch_1_x : torch.Tensor\n",
    "            Values of the first batch\n",
    "        batch_2_x : torch.Tensor\n",
    "            Values of the second batch\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # number of the samples in the dataset\n",
    "    num_of_samples = len(dataset)\n",
    "    \n",
    "    # size of the current batch\n",
    "\n",
    "    batch_size = num_of_samples-sample_idx\n",
    "    \n",
    "    # initializing the batches\n",
    "    batch_1_x = np.zeros((batch_size, 1, 28, 28))\n",
    "    batch_2_x = np.zeros((batch_size, 1, 28, 28))\n",
    "    \n",
    "    # index for the current batch\n",
    "    idx = 0\n",
    "    for i in range(sample_idx, num_of_samples):\n",
    "        # current batch contains the pairs of the current sample and the samples\n",
    "        # coming after the current one\n",
    "        batch_1_x[idx] = dataset[sample_idx][0]\n",
    "        batch_2_x[idx] = dataset[i][0]\n",
    "        idx += 1\n",
    "            \n",
    "    # casting the batches from numpy arrays to float tensors\n",
    "    batch_1_x = torch.from_numpy(batch_1_x).float()\n",
    "    batch_2_x = torch.from_numpy(batch_2_x).float()\n",
    "        \n",
    "    return batch_1_x, batch_2_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(dist_matrix, labels):\n",
    "    \"\"\"\n",
    "        Calculating Mean Average Precision given a symmetric pairwise distance matrix\n",
    "        and labels of the rows/columns\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dist_matrix : numpy.ndarray\n",
    "            Symmetric distance matrix that contains pairwise distances\n",
    "        labels : numpy.ndarray\n",
    "            Labels of all the samples in the dataset\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mAP : float\n",
    "            Mean average precision obtained from the distance matrix\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # number of samples in the dataset\n",
    "    num_of_samples = labels.size\n",
    "    \n",
    "    # initializing a matrix to store tuples of pairwise distances and labels of the reference samples\n",
    "    tuple_matrix = np.zeros((num_of_samples, num_of_samples), dtype=(float,2))\n",
    "    \n",
    "    # filling the tuple_matrix with distance values and labels\n",
    "    for i in range(num_of_samples):\n",
    "        for j in range(num_of_samples):\n",
    "            tuple_matrix[i][j] = (dist_matrix[i][j], labels[j])\n",
    "            \n",
    "    # initializing mAP\n",
    "    mAP = 0\n",
    "    \n",
    "    # calculating average precision for each row of the distance matrix\n",
    "    for i in range(num_of_samples):\n",
    "        # obtaining the current row\n",
    "        row = tuple_matrix[i]\n",
    "        \n",
    "        # label of the current query\n",
    "        label = labels[i]\n",
    "        \n",
    "        # sorting the row with respect to distance values\n",
    "        sorted_row = row[row[:,0].argsort()]\n",
    "        \n",
    "        # initializing true positive count \n",
    "        tp = 0\n",
    "        \n",
    "        # initializing precision value\n",
    "        prec = 0\n",
    "        \n",
    "        # counting number of instances that has the same label as the query\n",
    "        label_count = 0\n",
    "        \n",
    "        for j in range(1, num_of_samples):\n",
    "            # checking whether the reference sample has the same label as the query\n",
    "            if sorted_row[j][1] == label:\n",
    "                # incrementing the number of true positives\n",
    "                tp += 1\n",
    "                \n",
    "                # updating the precision value\n",
    "                prec += tp/j\n",
    "                \n",
    "                # incrementing the number of samples with the same label as the query\n",
    "                label_count += 1\n",
    "        \n",
    "        # updating  mAP\n",
    "        mAP += prec/label_count\n",
    "        \n",
    "    # updating mAP\n",
    "    mAP = mAP / num_of_samples\n",
    "    \n",
    "    return mAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# creating a folder named 'data' to store the dataset\n",
    "if not os.path.isdir('data/'):\n",
    "    os.mkdir('data/')\n",
    "\n",
    "# loading MNIST training set\n",
    "mnist_trainset = torchvision.datasets.MNIST('data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "# loading MNIST test set\n",
    "mnist_testset = torchvision.datasets.MNIST('data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the network and the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the siamese network\n",
    "siamese = Siamese()\n",
    "\n",
    "# initializing the optimization function\n",
    "optimizer = optim.Adam(siamese.parameters(), lr=0.001)\n",
    "\n",
    "# size of the training set\n",
    "dataset_size = len(mnist_trainset)\n",
    "\n",
    "# array of labels for training set\n",
    "# used for creating training batches\n",
    "idx_label_array = np.zeros(dataset_size, dtype='int')\n",
    "for i in range(dataset_size):\n",
    "    idx_label_array[i] = mnist_trainset[i][1].item()\n",
    "\n",
    "# size of training batches\n",
    "# note that actual batch size will be the double of this amount\n",
    "batch_size = 32\n",
    "\n",
    "# number of epochs to train\n",
    "num_of_epochs = 1\n",
    "\n",
    "# number of batches for each epoch\n",
    "num_of_batches = np.ceil(dataset_size/batch_size).astype('int')\n",
    "\n",
    "# genuine and impostor pairs for training\n",
    "# keeping track in order to not to give same pairs\n",
    "pair_set = set()\n",
    "\n",
    "# loss function values for visualization\n",
    "loss_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  0\n",
      "Current batch: 0/1875\n",
      "Contrastive loss: 1.6199431419372559\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 25/1875\n",
      "Contrastive loss: 0.4285477101802826\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 50/1875\n",
      "Contrastive loss: 0.46618005633354187\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 75/1875\n",
      "Contrastive loss: 0.48405417799949646\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 100/1875\n",
      "Contrastive loss: 0.44815507531166077\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 125/1875\n",
      "Contrastive loss: 0.34446218609809875\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 150/1875\n",
      "Contrastive loss: 0.2854847013950348\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 175/1875\n",
      "Contrastive loss: 0.30996373295783997\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 200/1875\n",
      "Contrastive loss: 0.297442227602005\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 225/1875\n",
      "Contrastive loss: 0.3142947554588318\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 250/1875\n",
      "Contrastive loss: 0.32156965136528015\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 275/1875\n",
      "Contrastive loss: 0.22850649058818817\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 300/1875\n",
      "Contrastive loss: 0.24661865830421448\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 325/1875\n",
      "Contrastive loss: 0.18790295720100403\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 350/1875\n",
      "Contrastive loss: 0.2298404723405838\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 375/1875\n",
      "Contrastive loss: 0.1798696666955948\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 400/1875\n",
      "Contrastive loss: 0.24542255699634552\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 425/1875\n",
      "Contrastive loss: 0.17640399932861328\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 450/1875\n",
      "Contrastive loss: 0.17000329494476318\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 475/1875\n",
      "Contrastive loss: 0.14300887286663055\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 500/1875\n",
      "Contrastive loss: 0.1925981491804123\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 525/1875\n",
      "Contrastive loss: 0.2064828872680664\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 550/1875\n",
      "Contrastive loss: 0.20027892291545868\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 575/1875\n",
      "Contrastive loss: 0.26527905464172363\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 600/1875\n",
      "Contrastive loss: 0.12605227530002594\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 625/1875\n",
      "Contrastive loss: 0.19386868178844452\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 650/1875\n",
      "Contrastive loss: 0.14745590090751648\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 675/1875\n",
      "Contrastive loss: 0.1892314851284027\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 700/1875\n",
      "Contrastive loss: 0.11393590271472931\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 725/1875\n",
      "Contrastive loss: 0.1725505292415619\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 750/1875\n",
      "Contrastive loss: 0.18105049431324005\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 775/1875\n",
      "Contrastive loss: 0.1887788474559784\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 800/1875\n",
      "Contrastive loss: 0.1339232623577118\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 825/1875\n",
      "Contrastive loss: 0.16503234207630157\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 850/1875\n",
      "Contrastive loss: 0.11250174045562744\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 875/1875\n",
      "Contrastive loss: 0.11596252024173737\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 900/1875\n",
      "Contrastive loss: 0.16694243252277374\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 925/1875\n",
      "Contrastive loss: 0.12156539410352707\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 950/1875\n",
      "Contrastive loss: 0.12510888278484344\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 975/1875\n",
      "Contrastive loss: 0.09581659734249115\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1000/1875\n",
      "Contrastive loss: 0.26118266582489014\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1025/1875\n",
      "Contrastive loss: 0.11224525421857834\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1050/1875\n",
      "Contrastive loss: 0.11693504452705383\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1075/1875\n",
      "Contrastive loss: 0.18742911517620087\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1100/1875\n",
      "Contrastive loss: 0.1276886761188507\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1125/1875\n",
      "Contrastive loss: 0.13065481185913086\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1150/1875\n",
      "Contrastive loss: 0.11873982846736908\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1175/1875\n",
      "Contrastive loss: 0.1441752165555954\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1200/1875\n",
      "Contrastive loss: 0.11361246556043625\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1225/1875\n",
      "Contrastive loss: 0.09225112944841385\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1250/1875\n",
      "Contrastive loss: 0.16207320988178253\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1275/1875\n",
      "Contrastive loss: 0.17320464551448822\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1300/1875\n",
      "Contrastive loss: 0.1672692745923996\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1325/1875\n",
      "Contrastive loss: 0.14517760276794434\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1350/1875\n",
      "Contrastive loss: 0.18811236321926117\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1375/1875\n",
      "Contrastive loss: 0.12901288270950317\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1400/1875\n",
      "Contrastive loss: 0.10692797601222992\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1425/1875\n",
      "Contrastive loss: 0.1771039515733719\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1450/1875\n",
      "Contrastive loss: 0.1487566977739334\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1475/1875\n",
      "Contrastive loss: 0.2176629900932312\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1500/1875\n",
      "Contrastive loss: 0.1599813848733902\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1525/1875\n",
      "Contrastive loss: 0.13470497727394104\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1550/1875\n",
      "Contrastive loss: 0.11572661250829697\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1575/1875\n",
      "Contrastive loss: 0.22658029198646545\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1600/1875\n",
      "Contrastive loss: 0.08602102845907211\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1625/1875\n",
      "Contrastive loss: 0.13176660239696503\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1650/1875\n",
      "Contrastive loss: 0.14848123490810394\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1675/1875\n",
      "Contrastive loss: 0.07909536361694336\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1700/1875\n",
      "Contrastive loss: 0.08431442826986313\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1725/1875\n",
      "Contrastive loss: 0.10970906913280487\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1750/1875\n",
      "Contrastive loss: 0.10631822049617767\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1775/1875\n",
      "Contrastive loss: 0.11789355427026749\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1800/1875\n",
      "Contrastive loss: 0.11754310876131058\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1825/1875\n",
      "Contrastive loss: 0.0969335287809372\n",
      "\n",
      "Current epoch:  0\n",
      "Current batch: 1850/1875\n",
      "Contrastive loss: 0.05064800754189491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training the network for number of epochs\n",
    "for epoch in range(num_of_epochs):\n",
    "    # for each batch in the current epoch\n",
    "    for batch_idx in range(int(num_of_batches)):\n",
    "        # start index of the current batch\n",
    "        start_idx = batch_idx*batch_size\n",
    "        # end index of the current batch\n",
    "        end_idx = min((batch_idx+1)*batch_size, dataset_size)\n",
    "        \n",
    "        # generating pairs for the current batch\n",
    "        batch_1_x, batch_2_x, batch_1_y, batch_2_y, batch_c, pair_set = generate_training_batch(mnist_trainset, \n",
    "                                                                                                idx_label_array, \n",
    "                                                                                                start_idx, \n",
    "                                                                                                end_idx, \n",
    "                                                                                                pair_set)\n",
    "        \n",
    "        # setting gradients of the optimizer to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # output of the first batch\n",
    "        res_1 = siamese(batch_1_x)\n",
    "        \n",
    "        # output of the second batch\n",
    "        res_2 = siamese(batch_2_x)\n",
    "        \n",
    "        # euclidean distance between pairs\n",
    "        dist = F.pairwise_distance(res_1, res_2)\n",
    "        \n",
    "        # contrastive loss function\n",
    "        contrastive_loss = torch.mean((1 - batch_c) * torch.pow(dist, 2)\n",
    "                                      + batch_c * torch.pow(torch.clamp(2 - dist, min=0.0), 2))\n",
    "        \n",
    "        # printing the loss for every 25 batch\n",
    "        if batch_idx%25 == 0:\n",
    "            loss_log.append(contrastive_loss.item())\n",
    "            print('Current epoch: ', epoch)\n",
    "            print('Current batch: {}/{}'.format(batch_idx, num_of_batches))\n",
    "            print('Contrastive loss: {}\\n'.format(contrastive_loss.item()))\n",
    "\n",
    "        # calculating gradients with backpropagation\n",
    "        contrastive_loss.backward()\n",
    "        \n",
    "        # updating the weights\n",
    "        optimizer.step()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the loss evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a subset of the MNIST test set\n",
    "test_subset = list()\n",
    "\n",
    "# size of the subset\n",
    "test_subset_size = 5000\n",
    "\n",
    "# array for labels of the subset\n",
    "test_subset_labels = np.zeros((test_subset_size))\n",
    "\n",
    "# generating the test subset and the labels\n",
    "for i in range(test_subset_size):\n",
    "    test_subset.append(mnist_testset[i])\n",
    "    test_subset_labels[i] = mnist_testset[i][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivating gradient tracking for testing\n",
    "with torch.no_grad():\n",
    "    # setting the model to evaluation mode\n",
    "    siamese.eval()\n",
    "    \n",
    "    # tensor for storing all the pairwise distances\n",
    "    dist_all = torch.tensor([])\n",
    "    \n",
    "    for i in range(test_subset_size):\n",
    "        # generating pairs for the current batch\n",
    "        batch_1_x, batch_2_x = generate_test_batch(test_subset, i)\n",
    "        \n",
    "        # output of the first batch\n",
    "        res_1 = siamese(batch_1_x)\n",
    "        \n",
    "        # output of the second batch\n",
    "        res_2 = siamese(batch_2_x)\n",
    "        \n",
    "        # euclidean distance between pairs\n",
    "        dist = F.pairwise_distance(res_1, res_2)\n",
    "        \n",
    "        # concatenating the current distance tensor with the previous distances\n",
    "        dist_all = torch.cat((dist_all, dist))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a symmetric distance matrix\n",
    "dist_matrix = np.zeros((test_subset_size, test_subset_size))\n",
    "\n",
    "# index counter for the distance tensor\n",
    "counter = 0\n",
    "\n",
    "for i in range(test_subset_size):\n",
    "    for j in range(i, test_subset_size):\n",
    "        \n",
    "        # filling the distance matrix with pairwise distances\n",
    "        dist_matrix[i][j] = dist_all[counter].item()\n",
    "        dist_matrix[j][i] = dist_all[counter].item()\n",
    "        counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision is  0.9743064962807318\n"
     ]
    }
   ],
   "source": [
    "# calculating mean average precision\n",
    "mAP = MAP(dist_matrix,test_subset_labels)\n",
    "print('Mean average precision is ', mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
